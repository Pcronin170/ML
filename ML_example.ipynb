{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Test\n",
    "Below are the instructions provided for this project:\n",
    "\n",
    "Objective:   Train a classification models to make prediction on testing data set, using the data in the “Sequence model data.zip” file. \n",
    "\n",
    "Note: \n",
    "1.\tThis is a sequence classification task, where the order of each feature matters. You could train a model without considering order as a baseline model, but must train a model addressing sequence because in real work, sequence analysis is part of the project.\n",
    "1.\tEach row represents a training/testing sample containing a sequence, where the first element is “PPD_197” and last element is “PPD_0”.\n",
    "1.\tAll the sequences have been padding, which is the reason why lots of zeros show up in “PPD_0”. \n",
    "1.\tAll the values in each entry is categorical variable. Imaging every value in the entry as an index of a word in natural language. \n",
    "1.\tY-variable is the first column called “LABEL”, in testing data, there is no label\n",
    "\n",
    "What we expect,\n",
    "1.\tA good prediction result, which we will compare with the hold out y variable in the testing data set\n",
    "1.\tThe process of how the prediction is made, including \n",
    "  1. create new features based on existing variables could be needed. \n",
    "  1. feature analysis \n",
    "  1. feature selection \n",
    "  1. model comparison\n",
    "  1. hyper-parameter tuning\n",
    "1.\tIf you could use library such as Keras, Tensorflow to train a deep neural network (DNN) classifier, that will be a very good plus, even if neural networks might not be the best performed model. \n",
    "You could use any tools available to you for this task. Ultimately, we will assess your work based on two criteria. \n",
    "  1. predictive accuracy on the test set using the PR-AUC metric, \n",
    "  1. model structure you finally applied, for example, we will consider how advance the model is, or if you could create additional meaningful features from the data we gave to you.\n",
    "1. You should return to us the following:\n",
    "  1. A 23,910 x 1 csv or txt file containing one prediction per line for each row in the test dataset.\n",
    "  1. A brief report describing the techniques you used to obtain the predictions, that at least should include the following parts: \n",
    "    1. why do you choose the model you use? \n",
    "    1. your estimates of predictive performance on the test data set, \n",
    "    1. some words telling us your understanding about the model you use. \n",
    "    1. The code for building the model, or the saved model such as pickle file.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "Below is the plan that I intend to fulfill\n",
    "\n",
    "1. Split data into training and testing sets\n",
    "1. Explore basic summary stats\n",
    "1. Create a data set for all key predictors 1, 2 and 3 word combinations\n",
    "  1. Create dictionary of dictionaries to do it (function)\n",
    "  1. Remove in frequent combinations\n",
    "  1. Create pandas dataframe with all the good predictors (function)\n",
    "1. Perform feature selection to reduce to the key features\n",
    "  1. Get it down to 100 features\n",
    "1. Run 3 models and try to tune some parameters: \n",
    "  1. SVM Linear\n",
    "  1. SVM\n",
    "  1. Decision Tree\n",
    "1. Validate on the testing sets and test the one with the best outcome to ensure that it's not overfit\n",
    "1. Run the prediction on the lock box training set for the top model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "Set up the libraries needed for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:50:10.885686Z",
     "start_time": "2019-04-02T01:50:08.580455Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.fixes import signature\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import tree\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "Create functions that will be used throughout the data process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:50:10.896687Z",
     "start_time": "2019-04-02T01:50:10.887686Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_phrases(df):\n",
    "    \"\"\"Create unique phrases from the raw data\n",
    "\n",
    "    Args:\n",
    "        df(df): dataframe with an 'id' 'label' and n features\n",
    "        \n",
    "    Kwargs:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: 'id', 'label', 'comb' combined features\n",
    "    \"\"\"   \n",
    "    # Create a dataframe of combined features with id, label, and concatenated phrases of concatenated words\n",
    "    df_comb = pd.DataFrame(df[['label']])\n",
    "\n",
    "    #Create list of all columns to concatenate\n",
    "    cols = df.columns.values.tolist()\n",
    "    cols.remove('label')\n",
    "    #cols.remove('id')\n",
    "\n",
    "    # Concatenate word to phrases and remove all zeros\n",
    "    df_comb['comb'] = df.loc[:, cols].apply(lambda x: '-'.join(x), axis=1)\n",
    "    df_comb['comb'].replace({'-0': ''}  ,inplace=True, regex=True)\n",
    "    df_comb['comb'].replace({'^[0]+-': ''},inplace=True, regex=True)\n",
    "    \n",
    "    return df_comb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## populate_phrase_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:50:10.913688Z",
     "start_time": "2019-04-02T01:50:10.899687Z"
    }
   },
   "outputs": [],
   "source": [
    "def populate_phrase_dict(df):\n",
    "    \"\"\"Create a dictonary of phrases per observation and records\n",
    "       phrase counts.\n",
    "\n",
    "    Args:\n",
    "        df(df): dataframe with 'id','label', and n features\n",
    "        phrases: dionary of phrases to include\n",
    "        \n",
    "    Kwargs:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: 'id', 'label', 'comb' combined features\n",
    "    \"\"\"   \n",
    "    phrase_d = {} \n",
    "    phrase_cnt_d = {}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        #Identify all unique 2 and 3 word combinations per row\n",
    "        r2 = re.findall(r\"\\d+-\\d+\",row['comb'])\n",
    "        r3 = re.findall(r\"\\d+-\\d+-\\d+\",row['comb'])\n",
    "        r4 = re.findall(r\"\\d+-\\d+-\\d+-\\d+\",row['comb'])\n",
    "        r = r2+r3+r4\n",
    "        r = list(set(r))\n",
    "\n",
    "        #Create a new key for the row id\n",
    "        phrase_d[index] = {}\n",
    "\n",
    "        #Populate dionaries for unique phrases and phrase counts by id\n",
    "        for j in r:\n",
    "            phrase_d[index][j] = 1\n",
    "            if j in phrase_cnt_d:\n",
    "                phrase_cnt_d[j] += 1\n",
    "            else:\n",
    "                phrase_cnt_d[j] = 1\n",
    "                \n",
    "    return phrase_d, phrase_cnt_d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## populate_tidy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:50:10.930690Z",
     "start_time": "2019-04-02T01:50:10.917689Z"
    }
   },
   "outputs": [],
   "source": [
    "def populate_tidy_data(phrase_d, phrase_l, df_raw):\n",
    "    \"\"\"Populate final tidy data set with phrases from list phrase_l\n",
    "\n",
    "    Args:\n",
    "        phrase_d(dict): dictionary of features per id\n",
    "        phrase_l(list): list of phrases to include\n",
    "        df(df): dataframe with 'label'\n",
    "        \n",
    "    Kwargs:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: 'label', 'comb' combined features\n",
    "    \"\"\"   \n",
    "    phrase_df = pd.DataFrame(columns=phrase_l, dtype=bool)\n",
    "    \n",
    "    #Add id and label to the table\n",
    "    train_tidy = pd.concat([df_raw[['label']],phrase_df], axis=1)\n",
    "    train_tidy = train_tidy.fillna(0)\n",
    "\n",
    "    # Identify columns used in the process\n",
    "    columns = list(train_tidy[phrase_l].columns)\n",
    "\n",
    "    # Populate train_tidy with features from phrase_d\n",
    "    for index, row in train_tidy.iterrows():\n",
    "        for col in columns:\n",
    "            if col in phrase_d[index]:\n",
    "                train_tidy.at[index,col] = 1\n",
    "                \n",
    "    return train_tidy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data\n",
    "Upload the raw data and create training and testing data sets\n",
    "\n",
    "**Output**\n",
    "* train_df: Training data set\n",
    "* test_df: Testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:50:14.354032Z",
     "start_time": "2019-04-02T01:50:10.933690Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create system variables from excel into script and review values in dictionary\n",
    "df = pd.read_csv('in/train.csv', dtype=str)\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# Split into training and testing data\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "#Drop df to save memory\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:50:14.366034Z",
     "start_time": "2019-04-02T01:50:14.356033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76635"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "19159"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confirm the training and testing is split 80:20\n",
    "len(train_df)\n",
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:50:16.411238Z",
     "start_time": "2019-04-02T01:50:14.369034Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.to_pickle(\"out/train_df.pkl\")\n",
    "test_df.to_pickle(\"out/test_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidy Training Data\n",
    "Create a tidy dataset where each column is a 1, 2 or 3 word phrase identified in the row that is above a certain count\n",
    "\n",
    "**Output**\n",
    "* train_tidy: Tidy data set with `id`, `label`, and ~1000 features to analyze\n",
    "* phrase_list: list of variables to include in the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create phrases\n",
    "Identify all permutations of 1, 2 and 3 word phrases in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:50:20.033600Z",
     "start_time": "2019-04-02T01:50:16.413238Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate all words into a single string\n",
    "df_comb = create_phrases(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:50:34.402037Z",
     "start_time": "2019-04-02T01:50:20.035601Z"
    }
   },
   "outputs": [],
   "source": [
    "# Identify all 2 and 3 word phrases\n",
    "(phrase_dict, phrase_cnt_dict) = populate_phrase_dict(df_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Thresholds\n",
    "Determine cut offs for frequency of phrases so that the future data frame so there is a resonable number of features.  The output of this is an empty dataframe with all of the phrases we are going to consider for the analysis.\n",
    "\n",
    "* All values that appeared at least 400 times were included as it created around 1000 features and was feasible to calcualte in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:50:35.003097Z",
     "start_time": "2019-04-02T01:50:34.406037Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create dataframe with counts of unique phrases per observation\n",
    "phrase_cnt_df = pd.DataFrame.from_dict(phrase_cnt_dict, orient='index')\n",
    "phrase_cnt_df.columns = ['cnt']\n",
    "phrase_cnt_df.cnt = pd.to_numeric(phrase_cnt_df.cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:50:35.013098Z",
     "start_time": "2019-04-02T01:50:35.005097Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1183"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A minimum of at least 400 rows having a phrase cuts the list down to around 1000 predictors which is\n",
    "# reasonable given the computational resources for this exercise\n",
    "phrase_list = phrase_cnt_df[phrase_cnt_df['cnt']>350].index.tolist()\n",
    "len(phrase_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate Tidy Data\n",
    "Populate train_tidy with the following:\n",
    "* id and label from train_df\n",
    "* True if the feature existed for that observation\n",
    "* Limited to the ~1000 most popular phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:51:34.539050Z",
     "start_time": "2019-04-02T01:50:35.015098Z"
    }
   },
   "outputs": [],
   "source": [
    "train_tidy=populate_tidy_data(phrase_dict, phrase_list, df_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:51:39.257522Z",
     "start_time": "2019-04-02T01:51:34.543051Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pickle data for easy reuse later\n",
    "train_tidy.to_pickle(\"out/train_tidy.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:51:40.502646Z",
     "start_time": "2019-04-02T01:51:39.260522Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reload train_tidy from pickle file\n",
    "train_tidy = pd.read_pickle(\"out/train_tidy.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "Identify the top 100 features based by using a chi-squarred test.\n",
    "\n",
    "**Output**\n",
    "* X_train(df): Training features\n",
    "* y_train(df): Training Outcome\n",
    "* X_features(list): List of final features to include in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:51:41.545751Z",
     "start_time": "2019-04-02T01:51:40.504647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Feature       Score\n",
      "1043    67-44  856.314669\n",
      "87       1-44  709.474551\n",
      "88       1-67  630.473944\n",
      "306     44-20  619.897800\n",
      "134      20-6  554.226243\n",
      "336       2-6  514.573747\n",
      "1        44-6  478.889323\n",
      "92        6-6  405.441630\n",
      "1159  1-67-44  352.361258\n",
      "996   1-44-20  345.612105\n"
     ]
    }
   ],
   "source": [
    "#Create training df and outcome vector\n",
    "X = train_tidy[phrase_list]\n",
    "y_train = pd.to_numeric(train_tidy.label)\n",
    "\n",
    "#Use SelectKBest to get the top 100 features\n",
    "kBestFeatures = SelectKBest(score_func=chi2, k=10)\n",
    "fit = kBestFeatures.fit(X,y_train)\n",
    "X_train = X[X.columns]\n",
    "\n",
    "#Review top 20 features for scores\n",
    "score_df = pd.DataFrame(fit.scores_)\n",
    "feature_df = pd.DataFrame(X.columns)\n",
    "feature_score_df = pd.concat([feature_df,score_df],axis=1)\n",
    "feature_score_df.columns = ['Feature','Score'] \n",
    "print(feature_score_df.nlargest(10,'Score')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:51:41.647761Z",
     "start_time": "2019-04-02T01:51:41.547751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76635, 50)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(76635,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X[feature_score_df[feature_score_df.index < 50].Feature.tolist()]\n",
    "\n",
    "# Confirm shape of X_train and y_train are correct\n",
    "X_train.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidy Testing Data\n",
    "Create the testing data set based on the data process to create the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:51:42.668863Z",
     "start_time": "2019-04-02T01:51:41.651761Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate all words into a single string\n",
    "test_comb_df = create_phrases(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:51:46.139210Z",
     "start_time": "2019-04-02T01:51:42.671863Z"
    }
   },
   "outputs": [],
   "source": [
    "# Identify all 2 and 3 word phrases\n",
    "(test_phrase_dict, test_phrase_cnt_dict) = populate_phrase_dict(test_comb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:52:00.996696Z",
     "start_time": "2019-04-02T01:51:46.141210Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create final testing tidy data set\n",
    "test_tidy= populate_tidy_data(test_phrase_dict, phrase_list, test_comb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:52:01.109707Z",
     "start_time": "2019-04-02T01:52:00.999696Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19159, 1183)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(19159,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create final data sets based on the variables we need\n",
    "X_test = test_tidy[phrase_list]\n",
    "y_test = pd.to_numeric(test_tidy['label'])\n",
    "X_test.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle Data\n",
    "Pickle the training and testing data sets so they can be reloaded in the future, and it can clear as much memory as possible so that it is saved for the future model runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:52:06.715267Z",
     "start_time": "2019-04-02T01:52:01.112707Z"
    }
   },
   "outputs": [],
   "source": [
    "#Pickle Data\n",
    "X.to_pickle(\"out/X.pkl\")\n",
    "X_train.to_pickle(\"out/X_train.pkl\")\n",
    "y_train.to_pickle(\"out/y_train.pkl\")\n",
    "X_test.to_pickle(\"out/X_test.pkl\")\n",
    "y_test.to_pickle(\"out/y_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:52:06.738270Z",
     "start_time": "2019-04-02T01:52:06.717268Z"
    }
   },
   "outputs": [],
   "source": [
    "del X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:52:07.002296Z",
     "start_time": "2019-04-02T01:52:06.741270Z"
    }
   },
   "outputs": [],
   "source": [
    "#Bring back in the data\n",
    "X_train = pd.read_pickle(\"out/X_train.pkl\")\n",
    "y_train = pd.read_pickle(\"out/y_train.pkl\")\n",
    "X_test  = pd.read_pickle(\"out/X_test.pkl\")\n",
    "y_test  = pd.read_pickle(\"out/y_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Models\n",
    "In the following code several models were trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Linear\n",
    "A Linear Support Vector machine model was trained an all features created (not just those limited by feature selection). The PR-AUC in the training data was quite good (0.48) and the testing data was close (0.44) implying that the model does not appear to have an issue with being overfit.  The parameter class_weight was set to balanced because this data set is an unbalanced problem and tweaking this parameter should give us the best chance of improving the PR-AUC metric when it is implemented in a real problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "Train the Support Vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:52:29.621558Z",
     "start_time": "2019-04-02T01:52:07.004296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear Support Vector Machine Model\n",
    "svmlinear_model = LinearSVC(class_weight='balanced')\n",
    "svmlinear_model.fit(X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:52:30.088605Z",
     "start_time": "2019-04-02T01:52:29.623558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision-recall score: 0.48\n"
     ]
    }
   ],
   "source": [
    "#Calculate PR-AUC\n",
    "y_score = svmlinear_model.decision_function(X)\n",
    "average_precision = average_precision_score(y_train, y_score)\n",
    "print('Average precision-recall score: {0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "Test the Support Vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:52:30.252621Z",
     "start_time": "2019-04-02T01:52:30.091605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision-recall score: 0.45\n"
     ]
    }
   ],
   "source": [
    "#Calculate PR-AUC\n",
    "y_score = svmlinear_model.decision_function(X_test)\n",
    "average_precision = average_precision_score(y_test, y_score)\n",
    "print('Average precision-recall score: {0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T01:52:30.384634Z",
     "start_time": "2019-04-02T01:52:30.255621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.73      0.81     15332\n",
      "          1       0.39      0.68      0.49      3827\n",
      "\n",
      "avg / total       0.80      0.72      0.74     19159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcualte other standard fit statistics\n",
    "y_pred = svmlinear_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "A support vector machine model was trained using the 100 features identified using feature selection.  Although the model does not appear overfit, the PR-AUC is much lower than SVM Linear or regression.  The model took 17 minutes to run so it's not feasible to tweak the tuning parameters or add additional variables using existing computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T02:12:57.255309Z",
     "start_time": "2019-04-02T01:52:30.386634Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear Support Vector Machine Model\n",
    "svm_model = SVC(class_weight='balanced')\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T02:19:07.265826Z",
     "start_time": "2019-04-02T02:12:57.257309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision-recall score: 0.27\n"
     ]
    }
   ],
   "source": [
    "#Calculate PR-AUC\n",
    "y_score = svm_model.decision_function(X_train)\n",
    "average_precision = average_precision_score(y_train, y_score)\n",
    "print('Average precision-recall score: {0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T02:20:43.884461Z",
     "start_time": "2019-04-02T02:19:07.267826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision-recall score: 0.26\n"
     ]
    }
   ],
   "source": [
    "#Calculate PR-AUC\n",
    "y_score = svm_model.decision_function(X_test[X_train.columns])\n",
    "average_precision = average_precision_score(y_test, y_score)\n",
    "print('Average precision-recall score: {0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "Decision trees have a propensity for overfitting and although I'm seeing a PR-AUC of 0.47 with the training data the testing data has a fit of 0.26 and this appears to be a very overfit model. It's possible that cutting down the predictors with better feature selection may help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T02:20:44.875560Z",
     "start_time": "2019-04-02T02:20:43.892461Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train Decision Tree model\n",
    "clf_model = tree.DecisionTreeClassifier()\n",
    "clf_model = clf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T02:20:44.962568Z",
     "start_time": "2019-04-02T02:20:44.878560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision-recall score: 0.45\n"
     ]
    }
   ],
   "source": [
    "# Calculate PR-AUC for training data\n",
    "y_score = clf_model.predict_proba(X_train)\n",
    "average_precision = average_precision_score(y_train, y_score[:,1])\n",
    "print('Average precision-recall score: {0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T02:20:44.998572Z",
     "start_time": "2019-04-02T02:20:44.965569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision-recall score: 0.24\n"
     ]
    }
   ],
   "source": [
    "#Calculate PR-AUC for testing data\n",
    "y_score = clf_model.predict_proba(X_test[X_train.columns])\n",
    "average_precision = average_precision_score(y_test, y_score[:,1])\n",
    "print('Average precision-recall score: {0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "Import the final testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T02:20:46.290662Z",
     "start_time": "2019-04-02T02:20:45.000572Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create system variables from excel into script and review values in dictionary\n",
    "final_df = pd.read_csv('in/test.csv', dtype=str)\n",
    "final_df.columns = final_df.columns.str.lower()\n",
    "final_df['label'] = 1 # need to add column to make functions work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Final Data\n",
    "Create the testing data set based on the data process to create the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T02:20:48.123832Z",
     "start_time": "2019-04-02T02:20:46.293662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23909, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate all words into a single string\n",
    "final_comb_df = create_phrases(final_df)\n",
    "final_comb_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T02:20:54.140433Z",
     "start_time": "2019-04-02T02:20:48.125832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23909"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "502525"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify all 2, 3, and 4 word phrases\n",
    "(final_phrase_dict, final_phrase_cnt_dict) = populate_phrase_dict(final_comb_df)\n",
    "len(final_phrase_dict.keys())\n",
    "len(final_phrase_cnt_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T02:21:12.379257Z",
     "start_time": "2019-04-02T02:20:54.142433Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create final finaling tidy data set\n",
    "final_tidy= populate_tidy_data(final_phrase_dict, phrase_list, final_comb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T02:21:12.513270Z",
     "start_time": "2019-04-02T02:21:12.381257Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23909, 1183)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop Fake added label so the prediction will run\n",
    "final_tidy.drop('label',axis=1, inplace=True)\n",
    "final_tidy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Outcome\n",
    "Make the final prediction of the outcome and export to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T02:21:12.667286Z",
     "start_time": "2019-04-02T02:21:12.517271Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create final data sets based on the variables we need\n",
    "y_predict = pd.DataFrame(svmlinear_model.predict(final_tidy))\n",
    "y_predict.columns = ['predict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T02:21:12.746294Z",
     "start_time": "2019-04-02T02:21:12.669286Z"
    }
   },
   "outputs": [],
   "source": [
    "#Export final predictions on testing set to CSV\n",
    "y_predict.to_csv(\"out/test_prediction.csv\", header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "265px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
