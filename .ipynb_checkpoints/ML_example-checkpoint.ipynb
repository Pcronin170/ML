{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Test\n",
    "Objective:   Train a classification models to make prediction on testing data set, using the data in the “Sequence model data.zip” file. \n",
    "\n",
    "Note: \n",
    "1.\tThis is a sequence classification task, where the order of each feature matters. You could train a model without considering order as a baseline model, but must train a model addressing sequence because in real work, sequence analysis is part of the project.\n",
    "1.\tEach row represents a training/testing sample containing a sequence, where the first element is “PPD_197” and last element is “PPD_0”.\n",
    "1.\tAll the sequences have been padding, which is the reason why lots of zeros show up in “PPD_0”. \n",
    "1.\tAll the values in each entry is categorical variable. Imaging every value in the entry as an index of a word in natural language. \n",
    "1.\tY-variable is the first column called “LABEL”, in testing data, there is no label\n",
    "\n",
    "What we expect,\n",
    "1.\tA good prediction result, which we will compare with the hold out y variable in the testing data set\n",
    "1.\tThe process of how the prediction is made, including \n",
    "  1. model comparison\n",
    "  1. hyper-parameter tuning\n",
    "  1. feature analysis \n",
    "  1. feature selection \n",
    "  1. create new features based on existing variables could be needed. \n",
    "1.\tIf you could use library such as Keras, Tensorflow to train a deep neural network (DNN) classifier, that will be a very good plus, even if neural networks might not be the best performed model. \n",
    "You could use any tools available to you for this task. Ultimately, we will assess your work based on two criteria. \n",
    "  1. predictive accuracy on the test set using the PR-AUC metric, \n",
    "  1. model structure you finally applied, for example, we will consider how advance the model is, or if you could create additional meaningful features from the data we gave to you.\n",
    "1. You should return to us the following:\n",
    "  1. A 23,910 x 1 csv or txt file containing one prediction per line for each row in the test dataset.\n",
    "  1. A brief report describing the techniques you used to obtain the predictions, that at least should include the following parts: \n",
    "    1. why do you choose the model you use? \n",
    "    1. your estimates of predictive performance on the test data set, \n",
    "    1. some words telling us your understanding about the model you use. \n",
    "    1. The code for building the model, or the saved model such as pickle file.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "Below is the plan that I intend to fulfill\n",
    "\n",
    "1. Split data into training and testing sets\n",
    "1. Explore basic summary stats\n",
    "1. Create a data set for all key predictors 1, 2 and 3 word combinations\n",
    "  1. Create dictionary of dictionaries to do it (function)\n",
    "  1. Remove in frequent combinations\n",
    "  1. Create pandas dataframe with all the good predictors (function)\n",
    "1. Perform feature selection to reduce to the key features\n",
    "  1. Get it down to 100 features\n",
    "1. Run 3 models and try to tune some parameters: \n",
    "  1. Logistic Regressions\n",
    "  1. Random forest\n",
    "  1. Support Vector machine\n",
    "1. Validate on the testing sets and test the one with the best outcome to ensure that it's not overfit\n",
    "1. Run the prediction on the lock box training set\n",
    "  1. Create a few summary stats on the prediction to ensure that mistakes weren't made\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "Set up the libraries needed for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T12:08:29.364962Z",
     "start_time": "2019-03-30T12:08:25.796759Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data\n",
    "Upload the raw data and create training and testing data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T13:21:46.791353Z",
     "start_time": "2019-03-30T13:21:40.480992Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create system variables from excel into script and review values in dictionary\n",
    "df = pd.read_csv('in/train.csv', dtype=str)\n",
    "df.columns = df.columns.str.lower()\n",
    "df['id'] = range(1, len(df) + 1)\n",
    "\n",
    "# Split into training and testing data\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "#Drop df to save memory\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T13:21:08.334154Z",
     "start_time": "2019-03-30T13:21:08.325153Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76635"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "19159"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)\n",
    "len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore data\n",
    "The first stage will be exploring the data.  Since each variable `PPD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are around 100,000 rows and approximately 20% of rows are the label. Since this is a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T12:09:14.325534Z",
     "start_time": "2019-03-30T12:09:12.302418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>frequency</th>\n",
       "      <th>pdf</th>\n",
       "      <th>cdf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>292490</td>\n",
       "      <td>0.044866</td>\n",
       "      <td>0.044866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>281438</td>\n",
       "      <td>0.043171</td>\n",
       "      <td>0.088038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>247951</td>\n",
       "      <td>0.038034</td>\n",
       "      <td>0.126072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>179368</td>\n",
       "      <td>0.027514</td>\n",
       "      <td>0.153586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>170400</td>\n",
       "      <td>0.026138</td>\n",
       "      <td>0.179724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  value  frequency       pdf       cdf\n",
       "0     2     292490  0.044866  0.044866\n",
       "1     1     281438  0.043171  0.088038\n",
       "2     3     247951  0.038034  0.126072\n",
       "3     4     179368  0.027514  0.153586\n",
       "4     6     170400  0.026138  0.179724"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frequency\n",
    "# stats_df = df_melt \\\n",
    "# .groupby('value') \\\n",
    "# ['value'] \\\n",
    "# .agg('count') \\\n",
    "# .pipe(pd.DataFrame) \\\n",
    "# .rename(columns = {'value': 'frequency'})\n",
    "# \n",
    "# stats_df = stats_df.sort_values('frequency', ascending=False)\n",
    "# \n",
    "# # PDF\n",
    "# stats_df['pdf'] = stats_df['frequency'] / sum(stats_df['frequency'])\n",
    "# \n",
    "# # CDF\n",
    "# stats_df['cdf'] = stats_df['pdf'].cumsum()\n",
    "# stats_df = stats_df.reset_index()\n",
    "# stats_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidy Data\n",
    "Create a tidy dataset where each column is a 1, 2 or 3 word phrase identified in the row that is above a certain count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create phrases\n",
    "Identify all permutations of 1, 2 and 3 word phrases in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T13:21:55.804869Z",
     "start_time": "2019-03-30T13:21:48.064426Z"
    }
   },
   "outputs": [],
   "source": [
    "df_comb = pd.DataFrame(train_df[['id','label']])\n",
    "\n",
    "cols = train_df.columns.values.tolist()\n",
    "cols.remove('label')\n",
    "cols.remove('id')\n",
    "\n",
    "df_comb['comb'] = train_df.loc[:, cols].apply(lambda x: '-'.join(x), axis=1)\n",
    "df_comb['comb'].replace({'-0': ''}  ,inplace=True, regex=True)\n",
    "df_comb['comb'].replace({'^[0]+-': ''},inplace=True, regex=True)\n",
    "\n",
    "#Just add 10 columns to get code right.  Will fix later\n",
    "#df_comb = df_comb.loc[df_comb.id < 10,['id','comb']]\n",
    "#len(df_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T13:47:47.208405Z",
     "start_time": "2019-03-30T13:46:18.197314Z"
    }
   },
   "outputs": [],
   "source": [
    "phrase_dict = {} \n",
    "phrase_cnt_dict = {}\n",
    "\n",
    "for index, row in df_comb.iterrows():\n",
    "    \n",
    "    #Identify all unique 1, 2, and 3 word combinations per row\n",
    "    r1 = re.findall(r\"\\d+\",row['comb'])\n",
    "    r2 = re.findall(r\"\\d+-\\d+\",row['comb'])\n",
    "    r3 = re.findall(r\"\\d+-\\d+-\\d+\",row['comb'])\n",
    "    r = r1+r2+r3\n",
    "    r = list(set(r))\n",
    "    \n",
    "    #Create a new key for the row id\n",
    "    phrase_dict[row['id']] = {}\n",
    "    \n",
    "    #Populate dictionaries for unique phrases and phrase counts by id\n",
    "    for j in r:\n",
    "        \n",
    "        phrase_dict[row['id']][j] = 1\n",
    "        \n",
    "        if (j) in phrase_cnt_dict:\n",
    "            phrase_cnt_dict[j] += 1\n",
    "        else:\n",
    "            phrase_cnt_dict[j] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Thresholds\n",
    "Determine cut offs for frequency of phrases so that the future data frame so there is a resonable number of features.  The output of this is an empty dataframe with all of the phrases we are going to consider for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T13:47:47.494421Z",
     "start_time": "2019-03-30T13:47:47.210405Z"
    }
   },
   "outputs": [],
   "source": [
    "phrase_cnt_df = pd.DataFrame.from_dict(phrase_cnt_dict, orient='index')\n",
    "phrase_cnt_df.columns = ['cnt']\n",
    "phrase_cnt_df.cnt = pd.to_numeric(phrase_cnt_df.cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T14:05:56.808299Z",
     "start_time": "2019-03-30T14:05:56.632289Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2033"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A minimum of at least 250 rows having a phrase cuts the list down to around 2000 predictors which is\n",
    "# reasonable given the computational resources for this exercise\n",
    "phrase_list = phrase_cnt_df[phrase_cnt_df['cnt']>250].index.tolist()\n",
    "phrase_df = pd.DataFrame(columns=phrase_list, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate Data\n",
    "Populate train_tidy with the following:\n",
    "* id and label from train_df\n",
    "* 1 or 0 if the feature occured in phrase_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T14:06:04.725531Z",
     "start_time": "2019-03-30T14:06:03.928486Z"
    }
   },
   "outputs": [],
   "source": [
    "#Add id and label to the table\n",
    "train_tidy = pd.concat([train_df[['id','label']],phrase_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loop to go through all the rows and columns and population with 1 or 0\n",
    "\n",
    "for each id in patinet\n",
    "    for each phrase in phrase_list\n",
    "        Set the value to 1 if it's in the dictionary and 0 if it's not\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate\n",
    "Validate the model on the testing data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "265px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
